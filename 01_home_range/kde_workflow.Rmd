---
title: "Caribou Kernal Density estimates"
author: "G Perkins"
date: "16/04/2020"
output: html_document
---

```{r copyright, include = FALSE}

# Copyright 2020 Province of British Columbia
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and limitations under the License.


```

# Summary

This workflow describes the process of generating Kernal density estimates for caribou herds, using R. This script is stored [here](https://github.com/bcgov/telemetry) along with other bits and pieces for analysing telemetry data. You can clone a copy directly from Github, or use the code.


## Initial set-up 

You will need to firstly install and R and R-studio. Detailed instructions can be found [here](https://github.com/bcgov/bcgov-data-science-resources/wiki). 
Once installed you can clone a copy directly from Github, or use the code directly. 

If you use the code in isolation it is recommended to use R-studio project to make the filepaths easy to set up. 

Save the raw data (csvs) in a folder called data. Note if new data is added please check the code to ensure consistency. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load in the libraries needed to run the analysis. Use  "install.packages()" command to install packages if needed. 

```{r load libraries , eval = TRUE}

library(lubridate)
library(dplyr)
library(readxl)
library(adehabitatHR)
library(sp)
library(ggplot2)
library(sf)
library(stringr)

```

set up your path to your data folder. 

```{r set up folders, eval = TRUE }

#list.files()
data_path <- file.path("../data")

```

As the raw data formats vary between herds, we will firstly run the kde's on the KlinseZa herd then the other herds in a loop. 

## KlinsaZa herd

Import the raw data and format the date variable so we can filter by months and years. We can explore the number of fixes per year

```{r read in KlinseZa, eval = TRUE}
indata <- read_xlsx(file.path(data_path, "KlinseZaAll_200320.xlsx"),
                    sheet = 2) %>%
                      rename(x = easting, y = northing )

# check data spread
counts.per.year = indata %>%
  group_by(yrbiol) %>%
  summarise(count = n())

counts.per.year

no.of.ids <- indata %>%
  group_by(animal_id)%>%
  summarise(fix.id = n())

```


The KlinseZa herd has data from `r min(counts.per.year$yrbiol)` to `r #max(counts.per.year$yrbiol)` from `r length(no.of.ids$animal_id)` animals. 

```{r, eval = FALSE}

# format date

counts.per.year = indata %>%
  group_by(yrbiol) %>%
  summarise(count = n())

# format date and filter for last 5 years and add season
#
# early winter : nov 1 - jan 14th
# late winter : Jan 15 - march 31
# spring : April 1 - May 14


indata <- indata %>%
  mutate(day = as.numeric(str_sub(date_time, 1,2)),
        month = str_sub(date_time, 3, 5)) %>%
  filter(yrbiol > 2014) %>%
  mutate(season = case_when(
            month == "APR" ~ "spring",
            month == "MAY" & day <15 ~ "spring",
            month %in% c("FEB", "MAR") ~ "late winter",
            month == "JAN" & day > 14 ~ "late winter",
            month %in% c("NOV", "DEC") ~ "early winter",
            month == "JAN" & day < 15 ~ "early winter"))

# filter seasons of interest

indata <- indata %>%
  filter(! is.na(season))


# filter animal 1d with > 50 counts

animal.ids <- indata %>%
  group_by(animal_id) %>%
  summarise(count = n()) %>%
  filter(count > 50) %>%
  dplyr:: select(animal_id) %>%
  pull()


indata <- indata %>%
  filter(animal_id %in% animal.ids) %>%
  mutate(animal_id = as.factor(animal_id))


# kernal density estimates

seasons = as.list(unique(indata$season))


for (s in seasons) {

  tdata <- indata %>%
    filter(season == s) %>%
    droplevels() %>%
    dplyr::select(x,y, animal_id) %>%
    distinct()

  # Create a SpatialPointsDataFrame by defining the coordinates
  coordinates(tdata) <- c("x", "y")
  proj4string(tdata) <- CRS( "+proj=utm +zone=10 +ellps=GRS80 +datum=NAD83 +units=m +no_defs" )
  tdfgeo <- spTransform(tdata, CRS("+init=epsg:3005")) # Transform to UTM


  # run KDE using href as the
  kde  <- kernelUD(tdfgeo, h = "href", kern = c("bivnorm"), grid = 500, extent = 2)

  ver95 <- getverticeshr( kde, 95)
  ver95.sf <- st_as_sf( ver95 )

  st_write(ver95.sf, file.path ("out", paste0("KlinseZa_KDE95_",s, "_href.shp")))


  # run KDE using href as the
  kde_lscv  <- kernelUD(tdfgeo, h = "LSCV", kern = c("bivnorm"), grid = 500, extent = 2)

  ver95_lscv<- getverticeshr( kde_lscv, 95)
  ver95_lscv.sf <- st_as_sf( ver95_lscv)

  st_write(ver95_lscv.sf , file.path ("out", paste0("KlinseZa_KDE95_",s, "_lsvc.shp")))


}


# FOr the othe herd that are all the same format


files <- list.files(data_path, pattern = "20191231.xlsx$")

for( f in files){

  f = files[2]
  fname = gsub("_20191231.xlsx", "", f)

# import all sheets into single file with name of year
indata <- read_xlsx(file.path(data_path, f)) %>%
  rename(x = AlbersX, y = AlbersY) %>%
  dplyr::select(Animal_ID, x, y, Year,	Month,	Day) %>%
  rename_all(.funs = tolower) %>%
  dplyr::filter(year > max(indata$year)-5) %>%
  mutate(season = case_when(
    month == 4 ~ "spring",
    month == 5 & day <15 ~ "spring",
    month %in% c(2, 3) ~ "late winter",
    month == 1 & day > 14 ~ "late winter",
    month %in% c(11, 12) ~ "early winter",
    month == 1 & day < 15 ~ "early winter")) %>%
  filter(!is.na(season))


# filter animal 1d with > 50 counts

animal.ids <- indata %>%
  group_by(animal_id) %>%
  summarise(count = n()) %>%
  filter(count > 50) %>%
  dplyr:: select(animal_id) %>%
  pull()


indata <- indata %>%
  filter(animal_id %in% animal.ids) %>%
  mutate(animal_id = as.factor(animal_id))


# kernal density estimates

seasons = as.list(unique(indata$season))


for (s in seasons) {

  s = seasons[[2]]

  tdata <- indata %>%
    filter(season == s) %>%
    droplevels() %>%
    dplyr::select(x,y, animal_id) %>%
    distinct()

  # Create a SpatialPointsDataFrame by defining the coordinates
  coordinates(tdata) <- c("x", "y")
  proj4string(tdata) <- CRS( "+proj=aea +lat_1=50 +lat_2=58.5 +lat_0=45 +lon_0=-126 +x_0=1000000 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs " )
  tdfgeo <- spTransform(tdata, CRS("+init=epsg:3005")) # Transform to UTM


  # run KDE using href as the
 #  kde  <- kernelUD(tdfgeo, h = "href", kern = c("bivnorm"), grid = 500, extent = 2)

 #  ver95 <- getverticeshr( kde, 95)
 #  ver95.sf <- st_as_sf( ver95 )

  #st_write(ver95.sf, file.path ("out", paste0(fname, "_KDE95_",s, "_href.shp")), overwrite = TRUE)


  # run KDE using href as the
  kde_lscv  <- kernelUD(tdfgeo, h = "LSCV", kern = c("bivnorm"), grid = 500, extent = 2)

  ver95_lscv<- getverticeshr( kde_lscv, 50)
  ver95_lscv.sf <- st_as_sf( ver95_lscv)

  st_write(ver95_lscv.sf , file.path ("out", paste0( fname, "_KDE50_",s, "_lsvc.shp")), overwrite = TRUE)


 }

}






## Kernel density estimates (KDE)

KDEs are very sensitive to input parameters, specfically the bandwidth (h) which determines the smoothing parameter (https://cran.r-project.org/web/packages/adehabitatHR/vignettes/adehabitatHR.pdf). H parameters can be estimated using three methods 1) a reference bandwidth (h = σ × n ^−1/6), however this is generally an overestimate of the range, and is not suitable for multi-modal distributions. 2) Least Square Cross Validation (LSCV), which minimises the difference in volumne between the true UD and the estimates UD. 3) A subjective visual choice for the smoothing parameter, based on successive trials (Silverman, 1986; Wand & Jones 1995).

To create home ranges for each unique id we used a bivariate normal kernel with a variety of h (smoothing parameters). We then interpreted visually to determine size to use based on successive trials. This is supported by the literature (Hemson et al. 2005; Calenge et al. 2011)

After extensive testing with a variety of h parameters we elected to have a user estimated h paramter. This followed implementation of similar Caribou Analysis conducted by T. Muhly <https://github.com/bcgov/clus/blob/master/R/caribou_habitat/04_caribou_habitat_model_telemetry_data_prep_doc.Rmd>




